{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with sqlite3.connect('dataset/Cleaned.db') as conn:\n",
    "    train = pd.read_sql_query('SELECT * FROM train', conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = train.is_duplicate\n",
    "y_true = list(map(int, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(train, y_true, stratify=y_true, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyemd import emd\n",
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format('/home/paperspace/w2v/GoogleNews-vectors-negative300.bin',\n",
    "                                         binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent2vec(sentence, model, method='tfidf', **kwargs):\n",
    "    \"\"\"\n",
    "    Generic function to convert a sentence to a vector using\n",
    "    avg or TFIDF vecorization.\n",
    "    \n",
    "    :param sentence: Sentence to be converted.\n",
    "    :param model: The word2vec model\n",
    "    \"\"\"\n",
    "    \n",
    "    ##### It is recommended to pass seperate stopwords #####\n",
    "    stopwords = kwargs.get('stopwords')\n",
    "    if stopwords is None:\n",
    "        from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "        stopwords = ENGLISH_STOP_WORDS\n",
    "    \n",
    "    ##### It is recommended to pass seperate tokenizers #####\n",
    "    tokenizer = kwargs.get('tokenizer')\n",
    "    if tokenizer is None:\n",
    "        from nltk.tokenize import RegexpTokenizer\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    words = tokenizer.tokenize(sentence) # Tokenize the words\n",
    "    words = {each for each in words if each not in stopwords} # Remove all the stopwords\n",
    "    \n",
    "    V = []\n",
    "    \n",
    "    for word in words: # Process over all the words in the sentence\n",
    "        if model.__contains__(word):\n",
    "            V.append(model[word])\n",
    "    V = np.array(V)\n",
    "    \n",
    "    # If no words were present in the model\n",
    "    # or blank sentence was passed, return a\n",
    "    # word vector with all 0's\n",
    "    if V.shape[0] == 0:\n",
    "        # If model returns word2vec of different size\n",
    "        # Default value is taken 300\n",
    "        custom_shape = kwargs.get('shape', 300)\n",
    "        return np.zeros(custom_shape)\n",
    "    \n",
    "    # If there is atleast one word in the sentence that\n",
    "    # was vectoried properly\n",
    "    \n",
    "    if method.lower() == 'avg':\n",
    "        V = V.sum(axis=0)\n",
    "        return V / np.sqrt((V ** 2).sum())\n",
    "    \n",
    "    elif method.lower() == 'tfidf':\n",
    "        tfidf_model = kwargs.get('tfidf_model') # Load the tfidf model\n",
    "        if tfidf_model: # If model loaded sucessfully\n",
    "            tfidf_vec = tfidf_model.transform([sentence]) # get TFIDF for the sentence\n",
    "            indx = tfidf_model.vocabulary_.get(word, -1)\n",
    "            tfidfs = []\n",
    "            for word in words:\n",
    "                if model.__contains__(word):\n",
    "                    if indx != -1:\n",
    "                        tfidfs.append(tfidf_vec[0, indx])\n",
    "                    else:\n",
    "                        tfidfs.append(0.0)\n",
    "            tfidfs = np.array(tfidfs)\n",
    "            denominator = tfidfs.sum()\n",
    "            if denominator == 0.0: # No word is representred in tfidf and w2v both\n",
    "                # Better than skipping that sentence\n",
    "                denominator = tfidf_model.idf_.min() * 0.01\n",
    "            numerator = V * tfidfs.reshape(V.shape[0], 1)\n",
    "            numerator = numerator.sum(axis=0)\n",
    "            return numerator / denominator\n",
    "        else:\n",
    "            raise ValueError('No tfidf model is present')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf.fit(Xtrain.question1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question1_vectors_train = np.zeros((Xtrain.shape[0], 300))\n",
    "for i, q in tqdm(enumerate(Xtrain.question1.values), total=283002):\n",
    "    question1_vectors_train[i, :] = sent2vec(q, model=model, tfidf_model=tfidf,\n",
    "                                       tokenizer=tokenizer, stopwords=ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question1_vectors_test = np.zeros((Xtest.shape[0], 300))\n",
    "for i, q in tqdm(enumerate(Xtest.question1.values), total=121287):\n",
    "    question1_vectors_test[i, :] = sent2vec(q, model=model, tfidf_model=tfidf,\n",
    "                                       tokenizer=tokenizer, stopwords=ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf.fit(Xtrain.question2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question2_vectors_train = np.zeros((Xtrain.shape[0], 300))\n",
    "for i, q in tqdm(enumerate(Xtrain.question2.values), total=283002):\n",
    "    question2_vectors_train[i, :] = sent2vec(q, model=model, tfidf_model=tfidf,\n",
    "                                       tokenizer=tokenizer, stopwords=ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question2_vectors_test = np.zeros((Xtest.shape[0], 300))\n",
    "for i, q in tqdm(enumerate(Xtest.question2.values), total=121287):\n",
    "    question2_vectors_test[i, :] = sent2vec(q, model=model, tfidf_model=tfidf,\n",
    "                                       tokenizer=tokenizer, stopwords=ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question1_vectors_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question2_vectors_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtrain.drop([\n",
    "    'id',\n",
    "    'qid1',\n",
    "    'qid2',\n",
    "    'question1',\n",
    "    'question2',\n",
    "    'is_duplicate'\n",
    "], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtest.drop([\n",
    "    'id',\n",
    "    'qid1',\n",
    "    'qid2',\n",
    "    'question1',\n",
    "    'question2',\n",
    "    'is_duplicate'\n",
    "], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.hstack((question1_vectors_train, question2_vectors_train, np.array(Xtrain)))\n",
    "X_test = np.hstack((question1_vectors_test, question2_vectors_test, np.array(Xtest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_depth': [2, 4, 6],  # How deep the base learners need to go -- typically small value\n",
    "    'subsample': [0.5, 0.75, 1.0], # Row sampling like Random Forest \n",
    "    'colsample_bytree': [0.3, 0.5, 0.7, 1.0]  # Column sampling like Random Forest\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator = xgb.XGBClassifier(random_state=42, objective='binary:logistic',  n_jobs=-1, n_estimators=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ls dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('dataset/w2v_train.npz', X_train)\n",
    "np.save('dataset/w2v_test.npz', X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.load('dataset/w2v_xtrain.npy')\n",
    "X_test = np.load('dataset/w2v_xtest.npy')\n",
    "ytrain = np.load('dataset/w2v_.ytrain.npy')\n",
    "ytest = np.load('dataset/w2v_ytest.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate & max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rates = [10, 1, 0.1, 0.01, 0.001]\n",
    "depths = [2, 4, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_losses = []\n",
    "for lr in learning_rates:\n",
    "    for dep in depths:\n",
    "        params = {}\n",
    "        params['objective'] = 'binary:logistic'\n",
    "        params['eval_metric'] = 'logloss'\n",
    "        params['eta'] = lr\n",
    "        params['max_depth'] = dep\n",
    "\n",
    "        d_train = xgb.DMatrix(X_train, label=ytrain)\n",
    "        d_test = xgb.DMatrix(X_test, label=ytest)\n",
    "\n",
    "        watchlist = [(d_train, 'train'), (d_test, 'valid')]\n",
    "\n",
    "        clf = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=20, verbose_eval=2)\n",
    "        predict_y = clf.predict(d_test)\n",
    "        log_losses.append(log_loss(ytest, predict_y, eps=1e-15))\n",
    "        print(f\"Done for lr = {lr} and depth = {dep}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.argsort(log_losses)[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_losses[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comb = []\n",
    "for lr in learning_rates:\n",
    "    for dep in depths:\n",
    "        comb.append((lr, dep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comb[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comb[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comb[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comb[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best Learning Rate:** 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that 0.1 is the best learning rate. Let's fine tune the max depth which decreses the validation loss but still seems to overfit least"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "depths = [1, 2, 3, 4, 5, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "for dep in depths:\n",
    "    params = {}\n",
    "    params['objective'] = 'binary:logistic'\n",
    "    params['eval_metric'] = 'logloss'\n",
    "    params['eta'] = 0.1\n",
    "    params['max_depth'] = dep\n",
    "    params['silent'] = 1\n",
    "#     silent=1\n",
    "\n",
    "    d_train = xgb.DMatrix(X_train, label=ytrain)\n",
    "    d_test = xgb.DMatrix(X_test, label=ytest)\n",
    "\n",
    "    watchlist = [(d_train, 'train'), (d_test, 'valid')]\n",
    "\n",
    "    clf = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=20, verbose_eval=1)\n",
    "    predict_y = clf.predict(d_test)\n",
    "    predict_y_train = clf.predict(d_train)\n",
    "    res = (dep, log_loss(ytrain, predict_y_train, eps=1e-15), log_loss(ytest, predict_y, eps=1e-15))\n",
    "    result.append(res)\n",
    "    print(f\"Done for depth = {dep}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = pd.DataFrame(result, columns=['max_depth', 'train_logloss', 'test_logloss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result['diff'] = abs(result.train_logloss - result.test_logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result.sort_values(by=['test_logloss', 'diff'], ascending=(True, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best `max_depth`:** 5 is the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am choosing `max_depth` of both 5 and 6. I believe a good param tuning on regularisation would do solve the little overfitting. Now let's fix the number of estimatiors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf.best_ntree_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'subsample': [0.5, 0.75, 1.0], # Row sampling like Random Forest \n",
    "    'colsample_bytree': [0.3, 0.5, 0.7, 1.0],  # Column sampling like Random Forest\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For `max_depth = 5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "for ss in [0.5, 0.75, 1.0]:\n",
    "    for cs in [0.3, 0.5, 0.7, 1.0]:\n",
    "        params = {}\n",
    "        params['objective'] = 'binary:logistic'\n",
    "        params['eval_metric'] = 'logloss'\n",
    "        params['eta'] = 0.1\n",
    "        params['max_depth'] = 5\n",
    "        params['silent'] = 1\n",
    "        params['subsample'] = ss\n",
    "        params['colsample_bytree'] = cs\n",
    "\n",
    "        d_train = xgb.DMatrix(X_train, label=ytrain)\n",
    "        d_test = xgb.DMatrix(X_test, label=ytest)\n",
    "\n",
    "        watchlist = [(d_train, 'train'), (d_test, 'valid')]\n",
    "\n",
    "        clf = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=20, verbose_eval=1)\n",
    "        predict_y = clf.predict(d_test)\n",
    "        predict_y_train = clf.predict(d_train)\n",
    "        res = (ss, cs, log_loss(ytrain, predict_y_train, eps=1e-15), log_loss(ytest, predict_y, eps=1e-15))\n",
    "        result.append(res)\n",
    "        print(f\"Done for subsample = {ss} and colsample_bytree = {cs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = pd.DataFrame(result, columns=['subsample', 'colsample_bytree', 'train_logloss', 'test_logloss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result['diff'] = abs(result.train_logloss - result.test_logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result.sort_values('diff', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally trying to remove some probable overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "for ra in [1e-3, 1e-2, 0.1, 1, 100]:\n",
    "    for rl in [1e-3, 1e-2, 0.1, 1, 100]:\n",
    "        params = {}\n",
    "        params['objective'] = 'binary:logistic'\n",
    "        params['eval_metric'] = 'logloss'\n",
    "        params['eta'] = 0.1\n",
    "        params['max_depth'] = 5\n",
    "        params['silent'] = 1\n",
    "        params['subsample'] = 1\n",
    "        params['colsample_bytree'] = 0.3\n",
    "        params['reg_alpha'] = ra\n",
    "        params['reg_lambda'] = rl\n",
    "\n",
    "        d_train = xgb.DMatrix(X_train, label=ytrain)\n",
    "        d_test = xgb.DMatrix(X_test, label=ytest)\n",
    "\n",
    "        watchlist = [(d_train, 'train'), (d_test, 'valid')]\n",
    "\n",
    "        clf = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=20, verbose_eval=1)\n",
    "        predict_y = clf.predict(d_test)\n",
    "        predict_y_train = clf.predict(d_train)\n",
    "        res = (ra, rl, log_loss(ytrain, predict_y_train, eps=1e-15), log_loss(ytest, predict_y, eps=1e-15))\n",
    "        result.append(res)\n",
    "        print(f\"Done for reg_alpha = {ra} and reg_lambda = {rl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = pd.DataFrame(result, columns=['reg_alpha', 'reg_lambda', 'train_logloss', 'test_logloss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result['diff'] = abs(result.train_logloss - result.test_logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result.sort_values(['diff', 'test_logloss'], ascending=(True, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best `reg_alpha` and `reg_lambda`:** 100, 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final loss for all best params:** 0.326"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/jturkewitz/magic-features-0-03-gain/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = train[['question1']].copy()\n",
    "\n",
    "df2 = train[['question2']].copy()\n",
    "df2.rename(columns = {'question2':'question1'},inplace=True)\n",
    "train_questions = df1.append(df2)\n",
    "train_questions.drop_duplicates(subset = ['question1'], inplace=True)\n",
    "questions_dict = pd.Series(train_questions.index.values, index=train_questions.question1.values).to_dict()\n",
    "train.drop(['qid1','qid2'], axis=1, inplace=True)\n",
    "train['q1_hash'] = train['question1'].map(questions_dict)\n",
    "train['q2_hash'] = train['question2'].map(questions_dict)\n",
    "q1_vc = train.q1_hash.value_counts().to_dict()\n",
    "q2_vc = train.q2_hash.value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def try_apply_dict(x, dict_to_apply):\n",
    "    try:\n",
    "        return dict_to_apply[x]\n",
    "    except KeyError:\n",
    "        return 0\n",
    "\n",
    "#map to frequency space\n",
    "train['q1_freq'] = train['q1_hash'].map(lambda x: try_apply_dict(x, q1_vc) + try_apply_dict(x, q2_vc))\n",
    "train['q2_freq'] = train['q2_hash'].map(lambda x: try_apply_dict(x, q1_vc) + try_apply_dict(x, q2_vc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_comb = train[['id', 'q1_hash', 'q2_hash', 'q1_freq', 'q2_freq', 'is_duplicate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corr_mat = train_comb.corr()\n",
    "corr_mat.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
